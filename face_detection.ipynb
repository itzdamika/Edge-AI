{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtflite_runtime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtflite\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# SETUP\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     10\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacenet.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "# -----------------------------\n",
    "# SETUP\n",
    "# -----------------------------\n",
    "MODEL_PATH = \"facenet.tflite\"\n",
    "EMBEDDINGS_DIR = \"face_embeddings\"\n",
    "FACE_DETECTION_SCALE = 1.3\n",
    "FACE_DETECTION_MIN_NEIGHBORS = 5\n",
    "MATCH_THRESHOLD = 0.7\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tflite.Interpreter(model_path=MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Ensure embeddings directory exists\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "# Load saved embeddings\n",
    "def load_embeddings():\n",
    "    embeddings = {}\n",
    "    for file in os.listdir(EMBEDDINGS_DIR):\n",
    "        if file.endswith(\".npy\"):\n",
    "            name = os.path.splitext(file)[0]\n",
    "            embeddings[name] = np.load(os.path.join(EMBEDDINGS_DIR, file))\n",
    "    return embeddings\n",
    "\n",
    "# Preprocess face image\n",
    "\n",
    "def preprocess_face(face_img):\n",
    "    face_resized = cv2.resize(face_img, (160, 160))\n",
    "    face_normalized = (face_resized / 127.5) - 1.0  # normalize to [-1, 1]\n",
    "    return face_normalized.astype(np.float32)\n",
    "\n",
    "# Get embedding from TFLite model\n",
    "def get_embedding(face_img):\n",
    "    input_data = np.expand_dims(face_img, axis=0)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data[0]\n",
    "\n",
    "# Cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "# Save embedding to file\n",
    "def save_embedding(name, embedding):\n",
    "    name = ''.join(c for c in name if c.isalnum())\n",
    "    path = os.path.join(EMBEDDINGS_DIR, f\"{name}.npy\")\n",
    "    np.save(path, embedding)\n",
    "    print(f\"Saved embedding for {name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN LOOP\n",
    "# -----------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "saved_embeddings = load_embeddings()\n",
    "save_mode = False\n",
    "input_name = \"\"\n",
    "\n",
    "print(\"Press 's' to save a new face, 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, FACE_DETECTION_SCALE, FACE_DETECTION_MIN_NEIGHBORS)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "        processed_face = preprocess_face(face_crop)\n",
    "        embedding = get_embedding(processed_face)\n",
    "\n",
    "        match_found = False\n",
    "        for name, saved_emb in saved_embeddings.items():\n",
    "            sim = cosine_similarity(embedding, saved_emb)\n",
    "            if sim > MATCH_THRESHOLD:\n",
    "                cv2.putText(frame, f\"{name} ({sim:.2f})\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            cv2.putText(frame, \"Unknown\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        if save_mode:\n",
    "            save_embedding(input_name, embedding)\n",
    "            saved_embeddings = load_embeddings()\n",
    "            save_mode = False\n",
    "            input_name = \"\"\n",
    "\n",
    "    cv2.imshow(\"Face Recognition (TFLite)\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('s'):\n",
    "        input_name = input(\"Enter name: \").strip()\n",
    "        if input_name:\n",
    "            save_mode = True\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "# from scipy.spatial.distance import cosine\n",
    "# from torchvision import transforms\n",
    "\n",
    "# # Initialize MTCNN (Face Detection)\n",
    "# mtcnn = MTCNN()\n",
    "\n",
    "# # Initialize Inception Resnet (Face Recognition)\n",
    "# model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# # Load the saved face embedding (if it exists)\n",
    "# saved_embedding = torch.load('my_face_embeddings.pth')\n",
    "\n",
    "# # Load the webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Threshold for face recognition (cosine distance threshold)\n",
    "# threshold = 0.6\n",
    "\n",
    "# # Image preprocessing pipeline\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((160, 160)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0, 0, 0], std=[255, 255, 255])  # Normalize to [-1, 1]\n",
    "# ])\n",
    "\n",
    "# # Function to recognize face by comparing embeddings\n",
    "# def recognize_face(face_tensor):\n",
    "#     # Get the face embedding\n",
    "#     embedding = model(face_tensor)\n",
    "    \n",
    "#     # Flatten both the saved embedding and the current face embedding to 1D\n",
    "#     embedding_flat = embedding.flatten()\n",
    "#     saved_embedding_flat = saved_embedding.flatten()\n",
    "    \n",
    "#     # Calculate the cosine distance between saved embedding and the current face embedding\n",
    "#     distance = cosine(saved_embedding_flat.detach().numpy(), embedding_flat.detach().numpy())\n",
    "#     print(f'Cosine distance: {distance}')\n",
    "    \n",
    "#     if distance < threshold:\n",
    "#         print(\"User recognized: Gaindu\")  # Print the recognized user on the console\n",
    "#         return \"Gaindu\"\n",
    "#     else:\n",
    "#         print(\"User recognized: Unknown\")  # Print \"Unknown\" on the console if not recognized\n",
    "#         return \"Unknown\"\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Detect faces\n",
    "#     faces, _ = mtcnn.detect(frame)\n",
    "    \n",
    "#     if faces is not None:\n",
    "#         for face in faces:\n",
    "#             # Draw bounding box around the detected face\n",
    "#             cv2.rectangle(frame, \n",
    "#                           (int(face[0]), int(face[1])), \n",
    "#                           (int(face[2]), int(face[3])), \n",
    "#                           (0, 255, 0), 2)\n",
    "            \n",
    "#             # Crop the face from the frame\n",
    "#             face_crop = frame[int(face[1]):int(face[3]), int(face[0]):int(face[2])]\n",
    "            \n",
    "#             if face_crop.size != 0:\n",
    "#                 # Preprocess the cropped face\n",
    "#                 face_tensor = preprocess(face_crop)\n",
    "\n",
    "#                 # Add batch dimension (for model compatibility)\n",
    "#                 face_tensor = face_tensor.unsqueeze(0)\n",
    "\n",
    "#                 # Recognize the face\n",
    "#                 label = recognize_face(face_tensor)\n",
    "\n",
    "#                 # Display the label on the image\n",
    "#                 cv2.putText(frame, label, (int(face[0]), int(face[1]) - 10), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "#         # Show the frame with bounding boxes and labels\n",
    "#         cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "#     # Exit when the 'Esc' key is pressed (key code 27)\n",
    "#     if cv2.waitKey(1) & 0xFF == 27:\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from scipy.spatial.distance import cosine\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import logging\n",
    "import gc\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# ===== Logging Setup =====\n",
    "logging.basicConfig(filename='intruder_detection.log', level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "logging.info(\"Starting intruder detection system.\")\n",
    "\n",
    "# ===== GPIO SETUP (Handles PC or Pi) =====\n",
    "try:\n",
    "    import RPi.GPIO as GPIO\n",
    "    ON_PI = True\n",
    "except (ImportError, RuntimeError):\n",
    "    logging.info(\"GPIO not available — running in mock mode.\")\n",
    "    ON_PI = False\n",
    "\n",
    "if ON_PI:\n",
    "    GPIO.setmode(GPIO.BCM)\n",
    "    GREEN_LED_PIN = 17\n",
    "    RED_LED_PIN = 27\n",
    "    GPIO.setup(GREEN_LED_PIN, GPIO.OUT)\n",
    "    GPIO.setup(RED_LED_PIN, GPIO.OUT)\n",
    "    GPIO.output(GREEN_LED_PIN, GPIO.LOW)\n",
    "    GPIO.output(RED_LED_PIN, GPIO.LOW)\n",
    "else:\n",
    "    GREEN_LED_PIN = RED_LED_PIN = None\n",
    "    def gpio_mock(*args, **kwargs): pass\n",
    "    GPIO = type('GPIO', (), {'output': gpio_mock, 'cleanup': gpio_mock})\n",
    "    logging.info(\"Running without physical GPIO.\")\n",
    "\n",
    "# ===== Initialize Face Detection and Recognition =====\n",
    "mtcnn = MTCNN(thresholds=[0.7, 0.8, 0.8])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "embeddings_dir = 'face_embeddings'\n",
    "\n",
    "# Set lower resolution for Raspberry Pi optimization\n",
    "CAP_WIDTH = 640\n",
    "CAP_HEIGHT = 480\n",
    "\n",
    "threshold = 0.5  # Stricter threshold for better unknown detection\n",
    "\n",
    "# Preprocessing steps to normalize and resize image for model input\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load saved face embeddings from disk\n",
    "def load_embeddings():\n",
    "    embeddings = {}\n",
    "    if os.path.exists(embeddings_dir):\n",
    "        for filename in os.listdir(embeddings_dir):\n",
    "            if filename.endswith('.pth'):\n",
    "                name = os.path.splitext(filename)[0]\n",
    "                path = os.path.join(embeddings_dir, filename)\n",
    "                embeddings[name] = torch.load(path, map_location=device)\n",
    "    logging.info(f\"Loaded {len(embeddings)} face embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "saved_embeddings = load_embeddings()\n",
    "\n",
    "# Prepare face tensor for recognition model\n",
    "def preprocess_face(face_crop):\n",
    "    face_resized = cv2.resize(face_crop, (160, 160))\n",
    "    face_tensor = preprocess(face_resized).unsqueeze(0)\n",
    "    return face_tensor\n",
    "\n",
    "# Recognize face by comparing embedding distances\n",
    "def recognize_face(face_tensor):\n",
    "    with torch.no_grad():\n",
    "        embedding = model(face_tensor.to(device)).detach().cpu().numpy()\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    recognized_name = \"Unknown\"\n",
    "\n",
    "    for name, saved_embedding in saved_embeddings.items():\n",
    "        saved_embedding = saved_embedding.cpu().numpy()\n",
    "        distance = cosine(embedding.flatten(), saved_embedding.flatten())\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            recognized_name = name\n",
    "\n",
    "    logging.info(f\"→ Closest match: {recognized_name}, Cosine distance: {min_distance:.4f}\")\n",
    "    return (recognized_name, min_distance) if min_distance < threshold else (\"Unknown\", min_distance)\n",
    "\n",
    "# ===== Motion Detection Setup =====\n",
    "motion_threshold = 50000  # Adjust based on your environment\n",
    "previous_frame = None\n",
    "\n",
    "# Global variable to store the frame with bounding boxes\n",
    "processed_frame = None\n",
    "frame_lock = threading.Lock()\n",
    "\n",
    "# Thread-safe queue to send frames for recognition\n",
    "frame_queue = queue.Queue(maxsize=2)\n",
    "\n",
    "# Thread worker to process motion and face recognition\n",
    "def recognition_worker():\n",
    "    global previous_frame, processed_frame\n",
    "    while True:\n",
    "        try:\n",
    "            frame = frame_queue.get(timeout=1)\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "\n",
    "        # Motion detection on a grayscale copy\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "\n",
    "        if previous_frame is None:\n",
    "            previous_frame = gray\n",
    "            continue\n",
    "\n",
    "        frame_delta = cv2.absdiff(previous_frame, gray)\n",
    "        thresh = cv2.threshold(frame_delta, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "        motion_score = np.sum(thresh)\n",
    "        previous_frame = gray\n",
    "\n",
    "        # Make a copy of the frame to draw on\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        if motion_score > motion_threshold:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Reset LED states\n",
    "            if ON_PI:\n",
    "                GPIO.output(GREEN_LED_PIN, GPIO.LOW)\n",
    "                GPIO.output(RED_LED_PIN, GPIO.LOW)\n",
    "\n",
    "            boxes, probs = mtcnn.detect(frame, landmarks=False)\n",
    "\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                valid_indices = [i for i, prob in enumerate(probs) if prob is not None and prob > 0.7]\n",
    "                valid_boxes = [boxes[i] for i in valid_indices]\n",
    "\n",
    "                for box in valid_boxes:\n",
    "                    x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "\n",
    "                    if x1 < x2 and y1 < y2:\n",
    "                        face_crop = frame[y1:y2, x1:x2]\n",
    "                        if face_crop.size > 0:\n",
    "                            try:\n",
    "                                face_tensor = preprocess_face(face_crop)\n",
    "                                label, distance = recognize_face(face_tensor)\n",
    "\n",
    "                                # Draw bounding box and label on the frame copy\n",
    "                                box_color = (0, 255, 0) if label != \"Unknown\" else (0, 0, 255)\n",
    "                                cv2.rectangle(display_frame, (x1, y1), (x2, y2), box_color, 2)\n",
    "                                label_text = f\"{label} ({distance:.2f})\"\n",
    "                                text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "                                cv2.rectangle(display_frame, \n",
    "                                              (x1, y1 - text_size[1] - 10), \n",
    "                                              (x1 + text_size[0], y1), \n",
    "                                              box_color, -1)\n",
    "                                cv2.putText(display_frame, label_text, (x1, y1 - 5), \n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                                # Handle LED status and logging\n",
    "                                if label == \"Unknown\":\n",
    "                                    logging.warning(\"[!] ALERT: Unrecognized face!\")\n",
    "                                    if ON_PI:\n",
    "                                        GPIO.output(RED_LED_PIN, GPIO.HIGH)\n",
    "                                    else:\n",
    "                                        logging.info(\"[MOCK] Red LED ON\")\n",
    "                                else:\n",
    "                                    logging.info(f\"[+] Recognized: {label}\")\n",
    "                                    if ON_PI:\n",
    "                                        GPIO.output(GREEN_LED_PIN, GPIO.HIGH)\n",
    "                                    else:\n",
    "                                        logging.info(f\"[MOCK] Green LED ON for {label}\")\n",
    "\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Error processing face: {e}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            cycle_time = end_time - start_time\n",
    "            logging.info(f\"Recognition cycle took {cycle_time:.4f} seconds\")\n",
    "            if cycle_time > 2.0:\n",
    "                logging.warning(\"Recognition cycle took longer than expected.\")\n",
    "        else:\n",
    "            logging.debug(\"Motion score too low, skipping recognition.\")\n",
    "\n",
    "        # Update the processed frame with lock to avoid race conditions\n",
    "        with frame_lock:\n",
    "            processed_frame = display_frame\n",
    "\n",
    "        # Perform garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "# Start the recognition thread\n",
    "recognition_thread = threading.Thread(target=recognition_worker, daemon=True)\n",
    "recognition_thread.start()\n",
    "\n",
    "# Main video capture loop\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, CAP_WIDTH)\n",
    "cap.set(4, CAP_HEIGHT)\n",
    "\n",
    "try:\n",
    "    logging.info(\"Starting video capture loop\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            logging.error(\"Failed to capture frame from camera.\")\n",
    "            break\n",
    "\n",
    "        # If queue is not full, add the frame for processing\n",
    "        if not frame_queue.full():\n",
    "            frame_queue.put(frame)\n",
    "\n",
    "        # Get the processed frame (with bounding boxes) if available\n",
    "        display_frame = None\n",
    "        with frame_lock:\n",
    "            if processed_frame is not None:\n",
    "                display_frame = processed_frame.copy()\n",
    "\n",
    "        # Display either the processed frame or the original\n",
    "        cv2.imshow('Live Feed', display_frame if display_frame is not None else frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # ESC key to exit\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging.info(\"KeyboardInterrupt received; shutting down.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if ON_PI:\n",
    "        GPIO.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from scipy.spatial.distance import cosine\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import logging\n",
    "import gc\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# ===== Logging Setup =====\n",
    "logging.basicConfig(filename='intruder_detection.log', level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "logging.info(\"Starting intruder detection system.\")\n",
    "\n",
    "# ===== GPIO SETUP =====\n",
    "try:\n",
    "    import RPi.GPIO as GPIO\n",
    "    ON_PI = True\n",
    "except (ImportError, RuntimeError):\n",
    "    logging.info(\"GPIO not available — running in mock mode.\")\n",
    "    ON_PI = False\n",
    "\n",
    "if ON_PI:\n",
    "    GPIO.setmode(GPIO.BCM)\n",
    "    GREEN_LED_PIN = 17\n",
    "    RED_LED_PIN = 27\n",
    "    GPIO.setup(GREEN_LED_PIN, GPIO.OUT)\n",
    "    GPIO.setup(RED_LED_PIN, GPIO.OUT)\n",
    "    GPIO.output(GREEN_LED_PIN, GPIO.LOW)\n",
    "    GPIO.output(RED_LED_PIN, GPIO.LOW)\n",
    "else:\n",
    "    GREEN_LED_PIN = RED_LED_PIN = None\n",
    "    def gpio_mock(*args, **kwargs): pass\n",
    "    GPIO = type('GPIO', (), {'output': gpio_mock, 'cleanup': gpio_mock})\n",
    "    logging.info(\"Running without physical GPIO.\")\n",
    "\n",
    "# ===== Initialize Face Detection and Recognition =====\n",
    "mtcnn = MTCNN(thresholds=[0.7, 0.8, 0.8])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "embeddings_dir = 'face_embeddings'\n",
    "CAP_WIDTH = 320  # Reduced resolution for smoother performance\n",
    "CAP_HEIGHT = 240  # Reduced resolution for smoother performance\n",
    "threshold = 0.5  # Cosine distance threshold\n",
    "\n",
    "# Preprocessing\n",
    "preprocess = transforms.Compose([ \n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# ===== Load TFLite-Compatible `.npy` Embeddings =====\n",
    "def load_embeddings():\n",
    "    embeddings = {}\n",
    "    if os.path.exists(embeddings_dir):\n",
    "        for filename in os.listdir(embeddings_dir):\n",
    "            if filename.endswith('.npy'):\n",
    "                name = os.path.splitext(filename)[0]\n",
    "                path = os.path.join(embeddings_dir, filename)\n",
    "                embeddings[name] = np.load(path)\n",
    "    logging.info(f\"Loaded {len(embeddings)} face embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "saved_embeddings = load_embeddings()\n",
    "\n",
    "def preprocess_face(face_crop):\n",
    "    face_resized = cv2.resize(face_crop, (160, 160))\n",
    "    face_tensor = preprocess(face_resized).unsqueeze(0)\n",
    "    return face_tensor\n",
    "\n",
    "def recognize_face(face_tensor):\n",
    "    with torch.no_grad():\n",
    "        embedding = model(face_tensor.to(device)).detach().cpu().numpy()\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    recognized_name = \"Unknown\"\n",
    "\n",
    "    for name, saved_embedding in saved_embeddings.items():\n",
    "        distance = cosine(embedding.flatten(), saved_embedding.flatten())\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            recognized_name = name\n",
    "\n",
    "    logging.info(f\"→ Closest match: {recognized_name}, Cosine distance: {min_distance:.4f}\")\n",
    "    return (recognized_name, min_distance) if min_distance < threshold else (\"Unknown\", min_distance)\n",
    "\n",
    "# ===== Motion Detection Setup =====\n",
    "motion_threshold = 50000\n",
    "previous_frame = None\n",
    "processed_frame = None\n",
    "frame_lock = threading.Lock()\n",
    "frame_queue = queue.Queue(maxsize=2)\n",
    "\n",
    "# ===== Worker Thread =====\n",
    "frame_counter = 0  # Frame counter for skipping frames\n",
    "frame_skip = 5  # Skip every 5th frame to reduce processing load\n",
    "\n",
    "def recognition_worker():\n",
    "    global previous_frame, processed_frame, frame_counter\n",
    "    while True:\n",
    "        try:\n",
    "            frame = frame_queue.get(timeout=1)\n",
    "        except queue.Empty:\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "\n",
    "        if previous_frame is None:\n",
    "            previous_frame = gray\n",
    "            continue\n",
    "\n",
    "        frame_delta = cv2.absdiff(previous_frame, gray)\n",
    "        thresh = cv2.threshold(frame_delta, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "        motion_score = np.sum(thresh)\n",
    "        previous_frame = gray\n",
    "\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        if motion_score > motion_threshold:\n",
    "            start_time = time.time()\n",
    "            if ON_PI:\n",
    "                GPIO.output(GREEN_LED_PIN, GPIO.LOW)\n",
    "                GPIO.output(RED_LED_PIN, GPIO.LOW)\n",
    "\n",
    "            boxes, probs = mtcnn.detect(frame, landmarks=False)\n",
    "\n",
    "            if boxes is not None:\n",
    "                valid_indices = [i for i, prob in enumerate(probs) if prob and prob > 0.7]\n",
    "                valid_boxes = [boxes[i] for i in valid_indices]\n",
    "\n",
    "                for box in valid_boxes:\n",
    "                    x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "\n",
    "                    if x1 < x2 and y1 < y2:\n",
    "                        face_crop = frame[y1:y2, x1:x2]\n",
    "                        if face_crop.size > 0:\n",
    "                            try:\n",
    "                                face_tensor = preprocess_face(face_crop)\n",
    "                                label, distance = recognize_face(face_tensor)\n",
    "\n",
    "                                color = (0, 255, 0) if label != \"Unknown\" else (0, 0, 255)\n",
    "                                cv2.rectangle(display_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                                label_text = f\"{label} ({distance:.2f})\"\n",
    "                                text_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "                                cv2.rectangle(display_frame,\n",
    "                                              (x1, y1 - text_size[1] - 10),\n",
    "                                              (x1 + text_size[0], y1),\n",
    "                                              color, -1)\n",
    "                                cv2.putText(display_frame, label_text, (x1, y1 - 5),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                                if label == \"Unknown\":\n",
    "                                    logging.warning(\"[!] ALERT: Unrecognized face!\")\n",
    "                                    if ON_PI:\n",
    "                                        GPIO.output(RED_LED_PIN, GPIO.HIGH)\n",
    "                                else:\n",
    "                                    logging.info(f\"[+] Recognized: {label}\")\n",
    "                                    if ON_PI:\n",
    "                                        GPIO.output(GREEN_LED_PIN, GPIO.HIGH)\n",
    "\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Error processing face: {e}\")\n",
    "\n",
    "            logging.info(f\"Recognition cycle took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "        with frame_lock:\n",
    "            processed_frame = display_frame\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# Start thread\n",
    "recognition_thread = threading.Thread(target=recognition_worker, daemon=True)\n",
    "recognition_thread.start()\n",
    "\n",
    "# ===== Video Capture Loop =====\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, CAP_WIDTH)\n",
    "cap.set(4, CAP_HEIGHT)\n",
    "\n",
    "try:\n",
    "    logging.info(\"Starting video capture loop\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            logging.error(\"Failed to capture frame from camera.\")\n",
    "            break\n",
    "\n",
    "        frame_counter += 1\n",
    "        # Skip frames to reduce detection frequency\n",
    "        if frame_counter % frame_skip != 0:\n",
    "            continue\n",
    "\n",
    "        if not frame_queue.full():\n",
    "            frame_queue.put(frame)\n",
    "\n",
    "        display_frame = None\n",
    "        with frame_lock:\n",
    "            if processed_frame is not None:\n",
    "                display_frame = processed_frame.copy()\n",
    "\n",
    "        # Display the frame with reduced resolution for faster processing\n",
    "        cv2.imshow('Live Feed', display_frame if display_frame is not None else frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # Exit on 'Esc' key\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging.info(\"KeyboardInterrupt received; shutting down.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if ON_PI:\n",
    "        GPIO.cleanup()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
